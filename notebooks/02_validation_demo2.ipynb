{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791f4f24",
   "metadata": {},
   "source": [
    "## 비정형 데이터 유효성 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d210cf",
   "metadata": {},
   "source": [
    "#### 파싱한 함수 위에 배치 나중에는 py에 넣어서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7b42237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR 파트\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 파일별 파싱 함수\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "INVOKE_URL = os.getenv(\"CLOVA_INVOKE_URL\")\n",
    "SECRET_KEY = os.getenv(\"CLOVA_OCR_SECRET\")\n",
    "\n",
    "# 파일(이미지/페이지)을 bytes 형태로 Clova OCR API에 보내서 OCR 결과 JSON을 받아옴\n",
    "def _call_clova_from_bytes(\n",
    "    filename: str,\n",
    "    file_bytes: bytes,\n",
    "    fmt: str,\n",
    "    lang: str = \"ko\",\n",
    "    enable_table: bool = True,\n",
    "    timeout: int = 180,\n",
    ") -> dict:\n",
    "    headers = {\"X-OCR-SECRET\": SECRET_KEY}\n",
    "    message = {\n",
    "        \"version\": \"V1\",\n",
    "        \"requestId\": f\"clova-{Path(filename).stem}-{int(time.time()*1000)}\",\n",
    "        \"timestamp\": int(time.time() * 1000),\n",
    "        \"lang\": lang,\n",
    "        \"images\": [{\"format\": fmt, \"name\": Path(filename).stem}],\n",
    "        \"enableTableDetection\": bool(enable_table),\n",
    "    }\n",
    "    files = {\n",
    "        \"file\": (filename, file_bytes, \"application/octet-stream\"),\n",
    "        \"message\": (None, json.dumps(message), \"application/json\"),\n",
    "    }\n",
    "    r = requests.post(INVOKE_URL, headers=headers, files=files, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "# PDF를 페이지별로 렌더링해서 PNG 이미지 바이트로 변환\n",
    "def _pdf_to_png_bytes_list(pdf_path: str, dpi: int = 200) -> list[tuple[str, bytes]]:\n",
    "    p = Path(pdf_path)\n",
    "    doc = fitz.open(str(p))\n",
    "    zoom = dpi / 72\n",
    "    mat = fitz.Matrix(zoom, zoom)\n",
    "\n",
    "    out: list[tuple[str, bytes]] = []\n",
    "    for i in range(len(doc)):\n",
    "        pix = doc[i].get_pixmap(matrix=mat, alpha=False)\n",
    "        out.append((f\"{p.stem}_p{i+1}.png\", pix.tobytes(\"png\")))\n",
    "    doc.close()\n",
    "    return out\n",
    "\n",
    "# Clova가 준 테이블 셀 1개에서 텍스트를 뽑는데, 셀 안에 여러 줄이면 줄바꿈(\\n)을 유지해서 합침\n",
    "def _cell_text_keep_lines(cell: dict) -> str:\n",
    "    lines: list[str] = []\n",
    "\n",
    "    if isinstance(cell.get(\"cellTextLines\"), list) and cell[\"cellTextLines\"]:\n",
    "        for ln in cell[\"cellTextLines\"]:\n",
    "            if isinstance(ln.get(\"cellWords\"), list) and ln[\"cellWords\"]:\n",
    "                s = \" \".join(\n",
    "                    [w.get(\"inferText\", \"\") for w in ln[\"cellWords\"] if w.get(\"inferText\")]\n",
    "                )\n",
    "            else:\n",
    "                s = ln.get(\"inferText\", \"\")\n",
    "            s = re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
    "            if s:\n",
    "                lines.append(s)\n",
    "\n",
    "    elif isinstance(cell.get(\"cellWords\"), list) and cell[\"cellWords\"]:\n",
    "        s = \" \".join([w.get(\"inferText\", \"\") for w in cell[\"cellWords\"] if w.get(\"inferText\")])\n",
    "        s = re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
    "        if s:\n",
    "            lines.append(s)\n",
    "\n",
    "    else:\n",
    "        s = (cell.get(\"inferText\") or cell.get(\"text\") or \"\")\n",
    "        s = re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
    "        if s:\n",
    "            lines.append(s)\n",
    "\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "# Clova OCR 결과 중에서 페이지의 일반 텍스트 영역(fields)을 전부 이어 붙여 페이지 전체 텍스트로 만듬\n",
    "def _page_text_from_fields(result_json: dict) -> str:\n",
    "    img0 = (result_json.get(\"images\") or [{}])[0]\n",
    "    fields = img0.get(\"fields\") or []\n",
    "    return \" \".join([f.get(\"inferText\", \"\") for f in fields if f.get(\"inferText\")]).strip()\n",
    "\n",
    "# 테이블에서 rowSpan / colSpan(병합셀) 정보를 보고, 병합된 셀의 텍스트를 병합 영역 전체 좌표에 복제해서 셀 리스트 만듬\n",
    "def _merge_cells_with_rowspan(cells: list[dict]) -> list[dict]:\n",
    "    has_span = any((\"rowSpan\" in c) or (\"colSpan\" in c) for c in cells)\n",
    "    if not has_span:\n",
    "        return cells\n",
    "\n",
    "    expanded_cells: list[dict] = []\n",
    "    for cell in cells:\n",
    "        row_idx = cell.get(\"rowIndex\", 0)\n",
    "        col_idx = cell.get(\"columnIndex\", 0)\n",
    "        row_span = cell.get(\"rowSpan\", 1) or 1\n",
    "        col_span = cell.get(\"colSpan\", 1) or 1\n",
    "        text = _cell_text_keep_lines(cell)\n",
    "\n",
    "        for r in range(row_idx, row_idx + row_span):\n",
    "            for c in range(col_idx, col_idx + col_span):\n",
    "                expanded_cells.append(\n",
    "                    {\n",
    "                        \"rowIndex\": r,\n",
    "                        \"columnIndex\": c,\n",
    "                        \"text\": text,\n",
    "                        \"isMerged\": (r != row_idx or c != col_idx),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return expanded_cells\n",
    "\n",
    "# Clova OCR 결과에서 tables를 꺼내서 2차원 배열(grid) 형태로 정리\n",
    "def _tables_to_schema(result_json: dict) -> dict:\n",
    "    img0 = (result_json.get(\"images\") or [{}])[0]\n",
    "    tables = img0.get(\"tables\") or []\n",
    "\n",
    "    out: dict = {}\n",
    "    for ti, t in enumerate(tables, start=1):\n",
    "        cells = t.get(\"cells\") or []\n",
    "        if not cells:\n",
    "            out[f\"table{ti}\"] = {\"rows\": [], \"rowCount\": 0, \"colCount\": 0}\n",
    "            continue\n",
    "\n",
    "        has_rc = any(\"rowIndex\" in c for c in cells) and any(\"columnIndex\" in c for c in cells)\n",
    "        if not has_rc:\n",
    "            texts = [_cell_text_keep_lines(c) for c in cells]\n",
    "            texts = [x for x in texts if x]\n",
    "            rows = [[x] for x in texts]\n",
    "            out[f\"table{ti}\"] = {\"rows\": rows, \"rowCount\": len(rows), \"colCount\": 1 if rows else 0}\n",
    "            continue\n",
    "\n",
    "        expanded_cells = _merge_cells_with_rowspan(cells)\n",
    "\n",
    "        max_r = max(c.get(\"rowIndex\", 0) for c in expanded_cells)\n",
    "        max_c = max(c.get(\"columnIndex\", 0) for c in expanded_cells)\n",
    "        grid = [[\"\"] * (max_c + 1) for _ in range(max_r + 1)]\n",
    "\n",
    "        for c in expanded_cells:\n",
    "            r = c.get(\"rowIndex\", 0)\n",
    "            col = c.get(\"columnIndex\", 0)\n",
    "            text = c.get(\"text\", \"\")\n",
    "            if not grid[r][col]:\n",
    "                grid[r][col] = text\n",
    "\n",
    "        out[f\"table{ti}\"] = {\n",
    "            \"rows\": grid,\n",
    "            \"rowCount\": len(grid),\n",
    "            \"colCount\": len(grid[0]) if grid else 0,\n",
    "        }\n",
    "\n",
    "    return out\n",
    "\n",
    "# 전체 파이프라인 엔트리 함수\n",
    "def clovaOCR(file_path: str, lang: str = \"ko\", dpi: int = 200, enable_table: bool = True) -> dict:\n",
    "    p = Path(file_path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(str(p.resolve()))\n",
    "\n",
    "    pages_out: dict = {}\n",
    "\n",
    "    if p.suffix.lower() == \".pdf\":\n",
    "        page_pngs = _pdf_to_png_bytes_list(str(p), dpi=dpi)\n",
    "\n",
    "        for i, (fname, png_bytes) in enumerate(page_pngs, start=1):\n",
    "            print(f\"[{i}/{len(page_pngs)}] OCR...\") # OCR 되는지 확인 용 디버깅\n",
    "            rj = _call_clova_from_bytes(\n",
    "                fname, png_bytes, fmt=\"png\", lang=lang, enable_table=enable_table\n",
    "            )\n",
    "            tables = _tables_to_schema(rj)\n",
    "\n",
    "            pages_out[f\"page{i}\"] = {\n",
    "                \"text\": _page_text_from_fields(rj),\n",
    "                \"tableCount\": len(tables),\n",
    "                \"tables\": tables,\n",
    "            }\n",
    "\n",
    "        return {\"pageCount\": len(page_pngs), \"pages\": pages_out}\n",
    "\n",
    "    with open(p, \"rb\") as f:\n",
    "        b = f.read()\n",
    "\n",
    "    fmt = p.suffix.lower().lstrip(\".\")\n",
    "    if fmt == \"jpeg\":\n",
    "        fmt = \"jpg\"\n",
    "\n",
    "    rj = _call_clova_from_bytes(p.name, b, fmt=fmt, lang=lang, enable_table=enable_table)\n",
    "    tables = _tables_to_schema(rj)\n",
    "\n",
    "    pages_out[\"page1\"] = {\n",
    "        \"text\": _page_text_from_fields(rj),\n",
    "        \"tableCount\": len(tables),\n",
    "        \"tables\": tables,\n",
    "    }\n",
    "    return {\"pageCount\": 1, \"pages\": pages_out}\n",
    "\n",
    "# 표처리 헬퍼\n",
    "def clean_table(tbl):   \n",
    "    return [\n",
    "        [c for c in (\"\" if c is None else str(c).strip() for c in row) if c]\n",
    "        for row in tbl\n",
    "        if any(\"\" if c is None else str(c).strip() for c in row)\n",
    "    ]\n",
    "\n",
    "\n",
    "CSV_EXT = {\"csv\"}\n",
    "EXCEL_EXT = {\"xls\", \"xlsx\"}\n",
    "IMAGE_EXT = {\"jpg\", \"png\"}\n",
    "OTHER_EXT = {\"pdf\", \"docx\"}\n",
    "\n",
    "def handle_csv(path: str, slotName: str):\n",
    "    for enc in [\"euc-kr\", \"cp949\", \"utf-8\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=enc)\n",
    "            return {\"slotName\": slotName, \"kind\": \"CSV\", \"ext\": \"csv\", \"dataframe\": df}\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise ValueError(\"인코딩 실패: cp949, euc-kr, utf-8 encoding을 지원합니다\")\n",
    "\n",
    "def handle_excel(path: str, ext: str, slotName: str):\n",
    "    df = pd.read_excel(path)\n",
    "    return {\"slotName\": slotName, \"kind\": \"EXCEL\", \"ext\": ext, \"dataframe\": df}\n",
    "\n",
    "def handle_image(path: str, ext: str, slotName: str):\n",
    "    ocr_output = clovaOCR(path)\n",
    "    return {\"slotName\": slotName, \"kind\": \"IMAGE\", \"ext\": ext, \"content\": ocr_output}\n",
    "\n",
    "def clean_table(tbl):\n",
    "    return [\n",
    "        [c for c in (\"\" if c is None else str(c).strip() for c in row) if c]\n",
    "        for row in tbl\n",
    "        if any(\"\" if c is None else str(c).strip() for c in row)\n",
    "    ]\n",
    "\n",
    "def handle_pdf(path: str, ext: str, slotName: str):\n",
    "    PDF_PAGE_MIN_CHARS = 30\n",
    "    PDF_PASS_RATIO = 0.7\n",
    "\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        total_pages = len(pdf.pages)\n",
    "        valid_page_count = 0\n",
    "        pages_data = {}\n",
    "\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            t = page.extract_text() or \"\"\n",
    "            clean_t = \" \".join(t.split()).strip()\n",
    "            if len(clean_t) >= PDF_PAGE_MIN_CHARS: valid_page_count += 1\n",
    "            raw = page.extract_tables() or []\n",
    "            tables = {f\"table{ti+1}\": (lambda rows: {\"rows\": rows, \"rowCount\": len(rows), \"colCount\": max((len(r) for r in rows), default=0)})(clean_table(tbl)) for ti, tbl in enumerate(raw)}\n",
    "            pages_data[f\"page{i+1}\"] = {\"text\": clean_t, \"tableCount\": len(raw), \"tables\": tables}\n",
    "\n",
    "        pass_ratio = valid_page_count / max(total_pages, 1)\n",
    "\n",
    "    if pass_ratio >= PDF_PASS_RATIO: return {\"slotName\": slotName, \"kind\":\"PDF\",\"ext\":ext,\"mode\":\"text\",\"pass_ratio\":pass_ratio,\"content\":{\"pageCount\":total_pages,\"pages\":pages_data}}\n",
    "    else:\n",
    "        ocr_output = clovaOCR(path)\n",
    "        return {\"slotName\": slotName, \"kind\": \"PDF\", \"ext\": ext, \"mode\": \"ocr\", \"content\": ocr_output}\n",
    "\n",
    "def handle_docx(path: str, ext: str, slotName: str):\n",
    "    doc = Document(path)\n",
    "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    tables = {}\n",
    "    for i, table in enumerate(doc.tables, start=1):\n",
    "        rows = [[cell.text for cell in row.cells] for row in table.rows]\n",
    "        tables[f\"table{i}\"] = {\n",
    "            \"rows\": rows,\n",
    "            \"rowCount\": len(rows),\n",
    "            \"colCount\": max((len(r) for r in rows), default=0),\n",
    "        }\n",
    "    pages = {\n",
    "        \"page1\": {\n",
    "            \"text\": \"\\n\".join(paragraphs),\n",
    "            \"tableCount\": len(doc.tables),\n",
    "            \"tables\": tables,\n",
    "        }\n",
    "    }\n",
    "    return {\"slotName\": slotName, \"kind\": \"DOCX\", \"ext\": ext, \"content\": {\"pageCount\": 1, \"pages\": pages}}\n",
    "\n",
    "# 파일 형식별 조건부 함수구동\n",
    "def handle_file(path: str, slotName: str):\n",
    "    ext = Path(path).suffix.lower().lstrip(\".\")\n",
    "    if ext in CSV_EXT: return handle_csv(path, slotName)\n",
    "    if ext in EXCEL_EXT: return handle_excel(path, ext, slotName)\n",
    "    if ext in IMAGE_EXT: return handle_image(path, ext, slotName)\n",
    "    if ext == \"pdf\": return handle_pdf(path, ext, slotName)\n",
    "    if ext == \"docx\": return handle_docx(path, ext, slotName)\n",
    "    raise ValueError(f\"지원하지 않는 파일 형식입니다: {ext}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab3aff",
   "metadata": {},
   "source": [
    "## 0) 목표 출력 포맷(네 요구 반영)<br>\n",
    "\n",
    "**PASS (비정형: payload 없어도 됨)**\n",
    "\n",
    "- status, file_name, validated_fields, processed_at 필수\n",
    "- payload는 있으면 추가, 비정형은 보통 생략\n",
    "\n",
    "**FAIL**\n",
    "\n",
    "- status, error, file_name, processed_at 필수\n",
    "- error.location은 비정형 기준으로 page/snippet 형태로 넣어주면 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e8997c",
   "metadata": {},
   "source": [
    "#### 1) (셀1) 슬롯 규칙 정의 (4~15번: slotName별 키워드/검증필드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ec4224f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from datetime import datetime, timezone\n",
    "import re\n",
    "\n",
    "# 공통 기본값(유효성 검증 기준)\n",
    "DEFAULT_REQUIRED_MIN_HITS = 1  # 키워드 최소 매칭 개수(3개 중 1개라도 있으면 통과)\n",
    "DEFAULT_MIN_TEXT_LEN = 80      # 텍스트 최소 길이(너무 짧으면 OCR/파싱 실패로 간주)\n",
    "\n",
    "# 공통 검증 체크 항목\n",
    "COMMON_VALIDATED_FIELDS = (\n",
    "    \"pageCount_valid\",       # 페이지 수가 0이 아닌지\n",
    "    \"text_present\",          # 텍스트가 비어있지 않은지\n",
    "    \"min_text_len_ok\",       # 텍스트 길이가 최소 기준 이상인지\n",
    "    \"keyword_min_hits_ok\",   # 키워드 규칙(최소 매칭 개수)을 만족하는지\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class SlotSpec:\n",
    "    slot_name: str\n",
    "    required_keywords: List[str]\n",
    "    required_min_hits: int = DEFAULT_REQUIRED_MIN_HITS\n",
    "    min_text_len: int = DEFAULT_MIN_TEXT_LEN\n",
    "\n",
    "    # 공통값 자동 주입 (슬롯에서 매번 validated_fields 넣을 필요 없음)\n",
    "    validated_fields: List[str] = field(default_factory=lambda: list(COMMON_VALIDATED_FIELDS))\n",
    "\n",
    "\n",
    "SLOT_SPECS: Dict[str, SlotSpec] = {\n",
    "    # 4 환경경영 인증 - ISO 14001 인증서\n",
    "    \"iso_14001_certificate\": SlotSpec(\n",
    "        slot_name=\"iso_14001_certificate\",\n",
    "        required_keywords=[\"ISO 14001\", \"Effective Date\", \"Expiration Date\"],\n",
    "    ),\n",
    "\n",
    "    # 5 대기오염 관리 - 대기 자가측정 기록부\n",
    "    \"air_self_measurement_log\": SlotSpec(\n",
    "        slot_name=\"air_self_measurement_log\",\n",
    "        required_keywords=[\"대기분야 측정기록부\", \"배출가스\", \"NOx\"],\n",
    "    ),\n",
    "\n",
    "    # 6 유해물질 관리 - MSDS\n",
    "    \"msds\": SlotSpec(\n",
    "        slot_name=\"msds\",\n",
    "        required_keywords=[\"물질안전보건자료\", \"Material Safety Data Sheet\", \"CAS\"],\n",
    "    ),\n",
    "\n",
    "    # 7 임직원 현황 - 인사현황표/임금대장\n",
    "    \"employee_status\": SlotSpec(\n",
    "        slot_name=\"employee_status\",\n",
    "        required_keywords=[\"임원 및 직원 등에 관한 사항\", \"임원 및 직원 등의 현황\", \"직원 등 현황\"],\n",
    "    ),\n",
    "\n",
    "    # 8 안전 교육 이수 - 안전보건교육 실시 결과 보고서\n",
    "    \"safety_training_report\": SlotSpec(\n",
    "        slot_name=\"safety_training_report\",\n",
    "        required_keywords=[\"안전보건 교육일지\", \"교육 대상자 수\", \"교육 실시자 수\"],\n",
    "    ),\n",
    "\n",
    "    # 9 산업재해 발생 - 산업재해 기록부/무재해 증명\n",
    "    \"industrial_accident_record\": SlotSpec(\n",
    "        slot_name=\"industrial_accident_record\",\n",
    "        required_keywords=[\"산업재해조사표\", \"산재관리번호\", \"재해발생원인\"],\n",
    "    ),\n",
    "\n",
    "    # 10 안전보건 인증 - ISO 45001 인증서\n",
    "    \"iso_45001_certificate\": SlotSpec(\n",
    "        slot_name=\"iso_45001_certificate\",\n",
    "        required_keywords=[\"ISO 45001\", \"Effective Date\", \"Expiration Date\"],\n",
    "    ),\n",
    "\n",
    "    # 11 근로권 준수 - 취업규칙/근로계약서 양식 (docx)\n",
    "    \"labor_rules_or_contract\": SlotSpec(\n",
    "        slot_name=\"labor_rules_or_contract\",\n",
    "        required_keywords=[\"근로계약\", \"근로 및 휴식시간\", \"임금 및 휴무\"],\n",
    "    ),\n",
    "\n",
    "    # 12 아동/강제노동 금지 - 인권 선언문/윤리강령 (docx)\n",
    "    \"no_child_forced_labor_policy\": SlotSpec(\n",
    "        slot_name=\"no_child_forced_labor_policy\",\n",
    "        required_keywords=[\"아동노동\", \"강제노동\", \"금지\"],\n",
    "    ),\n",
    "\n",
    "    # 13 윤리강령\n",
    "    \"code_of_ethics\": SlotSpec(\n",
    "        slot_name=\"code_of_ethics\",\n",
    "        required_keywords=[\"윤리강령\", \"부패\", \"신고\"],\n",
    "    ),\n",
    "\n",
    "    # 14 이사회 운영 현황 (사업보고서 내 이사회 파트)\n",
    "    \"board_operation\": SlotSpec(\n",
    "        slot_name=\"board_operation\",\n",
    "        required_keywords=[\"이사회\", \"사외이사\", \"주요 의결사항\"],\n",
    "    ),\n",
    "\n",
    "    # 15 이사의 보수 (사업보고서 내 임원의 보수 파트)\n",
    "    \"director_compensation\": SlotSpec(\n",
    "        slot_name=\"director_compensation\",\n",
    "        required_keywords=[\"임원의 보수\", \"보수총액\", \"산정기준 및 방법\"],\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618de640",
   "metadata": {},
   "source": [
    "#### 2) (셀2) 공통 유틸: 텍스트 정리 / 키워드 검사 / 스니펫 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "916c787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 시간을 UTC 기준 ISO 문자열(예: 2026-01-20T01:10:54Z)로 반환\n",
    "def now_iso_utc() -> str:\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# OCR/파싱 텍스트 정리: NBSP 제거 + 공백/줄바꿈을 한 칸으로 통일 + 양끝 공백 제거\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.replace(\"\\u00a0\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# 키워드 규칙 검사: keywords 중 최소 min_hits개 이상 발견되면 OK (hits/missing도 같이 반환)\n",
    "def keyword_min_hits(text: str, keywords: List[str], min_hits: int = 1) -> Tuple[bool, List[str], List[str]]:\n",
    "    t = (text or \"\").lower()\n",
    "    hits = [kw for kw in keywords if kw.lower() in t]\n",
    "    missing = [kw for kw in keywords if kw.lower() not in t]\n",
    "    ok = len(hits) >= (min_hits or 1)\n",
    "    return ok, hits, missing\n",
    "\n",
    "# 미리보기용 스니펫 생성: 텍스트를 정리한 뒤 max_len까지만 잘라서 반환\n",
    "def make_snippet(text: str, max_len: int = 180) -> str:\n",
    "    t = normalize_text(text)\n",
    "    if len(t) <= max_len:\n",
    "        return t\n",
    "    return t[:max_len] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3f116",
   "metadata": {},
   "source": [
    "#### 3) (셀3) 비정형 OCR JSON → 문서 요약(meta) 뽑기\n",
    "\n",
    "**너가 준 OCR/파싱 JSON 구조에 맞춰서:**\n",
    "\n",
    "- pageCount\n",
    "- page별 text 길이\n",
    "- tableCount\n",
    "- 전체 text 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ea367e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR/파싱 결과(JSON)에서 페이지/텍스트/테이블 정보를 요약(meta)으로 변환(검증용 full_text, page별 길이/스니펫 생성)\n",
    "def summarize_ocr_content(parsed: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    slot_name = parsed.get(\"slotName\", \"\") or \"\"\n",
    "    kind = parsed.get(\"kind\", \"\") or \"\"\n",
    "    ext = parsed.get(\"ext\", \"\") or \"\"\n",
    "    mode = parsed.get(\"mode\", \"\") or \"\"\n",
    "\n",
    "    content = parsed.get(\"content\", {}) or {}\n",
    "    page_count = int(content.get(\"pageCount\", 0) or 0)\n",
    "    pages = (content.get(\"pages\", {}) or {})\n",
    "\n",
    "    page_summaries: List[Dict[str, Any]] = []\n",
    "    full_text_parts: List[str] = []\n",
    "    total_table_count = 0\n",
    "\n",
    "    # page1, page2, ... 순서대로 정렬\n",
    "    def page_sort_key(k: str) -> int:\n",
    "        m = re.search(r\"(\\d+)$\", k)\n",
    "        return int(m.group(1)) if m else 999999\n",
    "\n",
    "    for page_key in sorted(pages.keys(), key=page_sort_key):\n",
    "        page_obj = pages.get(page_key, {}) or {}\n",
    "\n",
    "        page_text = page_obj.get(\"text\", \"\") or \"\"\n",
    "        page_text_norm = normalize_text(page_text)\n",
    "\n",
    "        if page_text_norm:\n",
    "            full_text_parts.append(page_text_norm)\n",
    "\n",
    "        table_count = int(page_obj.get(\"tableCount\", 0) or 0)\n",
    "        total_table_count += table_count\n",
    "\n",
    "        page_summaries.append({\n",
    "            \"page\": page_key,\n",
    "            \"text_len\": len(page_text_norm),\n",
    "            \"tableCount\": table_count,\n",
    "            \"snippet\": make_snippet(page_text_norm),\n",
    "        })\n",
    "\n",
    "    # 전체 텍스트 합치기 + 한번 더 normalize\n",
    "    full_text = normalize_text(\" \".join(full_text_parts))\n",
    "    total_text_len = len(full_text)\n",
    "\n",
    "    # 디버깅 편의: 텍스트가 가장 긴 페이지(대표 페이지) 하나 뽑기\n",
    "    top_page = None\n",
    "    if page_summaries:\n",
    "        top_page = max(page_summaries, key=lambda x: x.get(\"text_len\", 0))\n",
    "\n",
    "    return {\n",
    "        \"slotName\": slot_name,\n",
    "        \"kind\": kind,\n",
    "        \"ext\": ext,\n",
    "        \"mode\": mode,\n",
    "        \"pageCount\": page_count,\n",
    "        \"total_text_len\": total_text_len,\n",
    "        \"total_table_count\": total_table_count,\n",
    "        \"has_table\": total_table_count > 0,   # (선택) 나중에 쓰기 편함\n",
    "        \"pages\": page_summaries,\n",
    "        \"top_page\": top_page,                # (중요) 실패 시 근거로 쓰기 좋음\n",
    "        \"full_text\": full_text,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69bc297",
   "metadata": {},
   "source": [
    "#### 4) (셀4) 핵심: 슬롯별 비정형 유효성 검증 함수\n",
    "\n",
    "**검증 규칙(너가 말한 최소 기준 반영):**\n",
    "\n",
    "- pageCount 존재/정상\n",
    "- text 비어있지 않음 (전체 텍스트 길이, 첫 페이지 길이)\n",
    "- 필수 키워드 포함 (slotName별)\n",
    "- PASS/FAIL 결과 포맷 맞추기\n",
    "    - 비정형은 payload 생략 가능\n",
    "    - validated_fields, processed_at 항상 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a4376f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASS 결과 포맷 생성(비정형은 payload 생략 가능, validated_fields/processed_at은 항상 포함)\n",
    "def make_pass(file_name: str, validated_fields: List[str], payload: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    res = {\n",
    "        \"status\": \"PASS\",\n",
    "        \"file_name\": file_name,\n",
    "        \"validated_fields\": validated_fields,\n",
    "        \"processed_at\": now_iso_utc()\n",
    "    }\n",
    "    if payload:\n",
    "        res[\"payload\"] = payload\n",
    "    return res\n",
    "\n",
    "\n",
    "# FAIL 결과 포맷 생성(에러 코드/메시지 + 근거 위치(location) 포함)\n",
    "def make_fail(\n",
    "    file_name: str,\n",
    "    code: str,\n",
    "    message: str,\n",
    "    location: Optional[str] = None,\n",
    "    issues: Optional[List[Dict[str, Any]]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    err = {\"code\": code, \"message\": message}\n",
    "    if location:\n",
    "        err[\"location\"] = location\n",
    "    if issues:\n",
    "        err[\"issues\"] = issues  # 여러 문제를 한 번에 전달\n",
    "\n",
    "    return {\n",
    "        \"status\": \"FAIL\",\n",
    "        \"error\": err,\n",
    "        \"file_name\": file_name,\n",
    "        \"processed_at\": now_iso_utc()\n",
    "    }\n",
    "\n",
    "# 슬롯별 스펙(SLOT_SPECS)에 따라 OCR/파싱 결과를 최소 기준(page/text/키워드)로 PASS/FAIL 판정\n",
    "def validate_unstructured_slot(parsed: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    summary = summarize_ocr_content(parsed)\n",
    "    slot_name = summary.get(\"slotName\", \"\") or \"unknown_slot\"\n",
    "\n",
    "    # 1) 스펙 존재 여부\n",
    "    spec = SLOT_SPECS.get(slot_name)\n",
    "    if spec is None:\n",
    "        return make_fail(\n",
    "            file_name=slot_name,\n",
    "            code=\"UNKNOWN_SLOT\",\n",
    "            message=f\"등록되지 않은 slotName 입니다: {slot_name}\",\n",
    "            location=\"slotName\"\n",
    "        )\n",
    "\n",
    "    # 대표 페이지(근거용): top_page 있으면 그걸 사용, 없으면 첫 페이지\n",
    "    ref_page = summary.get(\"top_page\") or (summary[\"pages\"][0] if summary.get(\"pages\") else None)\n",
    "    ref_loc = None\n",
    "    if ref_page:\n",
    "        ref_loc = f'{ref_page[\"page\"]}:snippet=\"{ref_page.get(\"snippet\",\"\")}\"'\n",
    "\n",
    "    # 여러 문제를 한 번에 모으기\n",
    "    problems: List[str] = []\n",
    "\n",
    "    # 2) 기본 구조 검사\n",
    "    if summary.get(\"pageCount\", 0) <= 0:\n",
    "        problems.append(\"EMPTY_PAGECOUNT: pageCount가 0이거나 누락되어 문서를 처리할 수 없습니다.\")\n",
    "\n",
    "    if not summary.get(\"pages\"):\n",
    "        problems.append(\"EMPTY_PAGES: pages 정보가 없어 문서를 처리할 수 없습니다.\")\n",
    "\n",
    "    # 3) 텍스트 존재성 검사\n",
    "    total_len = summary.get(\"total_text_len\", 0) or 0\n",
    "    if total_len == 0:\n",
    "        problems.append(\"EMPTY_TEXT: 텍스트가 비어있습니다(OCR/파싱 실패 가능).\")\n",
    "    elif total_len < spec.min_text_len:\n",
    "        problems.append(f\"TEXT_TOO_SHORT: OCR 텍스트가 너무 짧습니다(총 {total_len}자 < 최소 {spec.min_text_len}자).\")\n",
    "\n",
    "    # 4) 필수 키워드 검사\n",
    "    ok_kw, hits, missing = keyword_min_hits(\n",
    "        text=summary.get(\"full_text\", \"\"),\n",
    "        keywords=spec.required_keywords,\n",
    "        min_hits=spec.required_min_hits\n",
    "    )\n",
    "    if not ok_kw:\n",
    "        problems.append(f\"필수 키워드 매칭 실패(최소 {spec.required_min_hits}개 필요): 누락={missing}\")\n",
    "\n",
    "    # 문제 있으면 FAIL 1번만 반환(메시지에 전부 포함)\n",
    "    if problems:\n",
    "        return make_fail(\n",
    "            file_name=slot_name,\n",
    "            code=\"VALIDATION_FAILED\",\n",
    "            message=\"; \".join(problems),\n",
    "            location=ref_loc or \"content.pages\"\n",
    "        )\n",
    "\n",
    "    # PASS (비정형은 payload 생략)\n",
    "    return make_pass(\n",
    "        file_name=slot_name,\n",
    "        validated_fields=spec.validated_fields,\n",
    "        payload=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5284c3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'PASS',\n",
       " 'file_name': 'iso_14001_certificate',\n",
       " 'validated_fields': ['pageCount_valid',\n",
       "  'text_present',\n",
       "  'min_text_len_ok',\n",
       "  'keyword_min_hits_ok'],\n",
       " 'processed_at': '2026-01-20T04:47:28Z'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/samples/E/성광벤드_ISO_14001.pdf\"\n",
    "slot_name = \"iso_14001_certificate\"\n",
    "\n",
    "parsed = handle_file(path, slot_name)\n",
    "parsed\n",
    "\n",
    "validation = validate_unstructured_slot(parsed)\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f8aa0045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'PASS',\n",
       " 'file_name': 'air_self_measurement_log',\n",
       " 'validated_fields': ['pageCount_valid',\n",
       "  'text_present',\n",
       "  'min_text_len_ok',\n",
       "  'keyword_min_hits_ok'],\n",
       " 'processed_at': '2026-01-20T04:47:31Z'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/samples/E/성광벤드_대기 측정기록부(환경분야 시험ㆍ검사 등에 관한 법률 시행규칙).pdf\"\n",
    "slot_name = \"air_self_measurement_log\"\n",
    "\n",
    "parsed = handle_file(path, slot_name)\n",
    "parsed\n",
    "\n",
    "validation = validate_unstructured_slot(parsed)\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "76aabb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'PASS',\n",
       " 'file_name': 'labor_rules_or_contract',\n",
       " 'validated_fields': ['pageCount_valid',\n",
       "  'text_present',\n",
       "  'min_text_len_ok',\n",
       "  'keyword_min_hits_ok'],\n",
       " 'processed_at': '2026-01-20T04:47:31Z'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/samples/S/근로계약서_서약서_지적사항강화_샘플.docx\"\n",
    "slot_name = \"labor_rules_or_contract\"\n",
    "\n",
    "parsed = handle_file(path, slot_name)\n",
    "parsed\n",
    "\n",
    "validation = validate_unstructured_slot(parsed)\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "893acbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'PASS',\n",
       " 'file_name': 'no_child_forced_labor_policy',\n",
       " 'validated_fields': ['pageCount_valid',\n",
       "  'text_present',\n",
       "  'min_text_len_ok',\n",
       "  'keyword_min_hits_ok'],\n",
       " 'processed_at': '2026-01-20T04:47:31Z'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/samples/S/윤리강령_샘플_아동강제노동포함.docx\"\n",
    "slot_name = \"no_child_forced_labor_policy\"\n",
    "\n",
    "parsed = handle_file(path, slot_name)\n",
    "parsed\n",
    "\n",
    "validation = validate_unstructured_slot(parsed)\n",
    "validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd456a2",
   "metadata": {},
   "source": [
    "file_name: 현재 slot_name이라서 demo에서는 그대로 이제 파이프라인 연결때 파일 이름으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e9b1bd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'FAIL',\n",
       " 'error': {'code': 'VALIDATION_FAILED',\n",
       "  'message': 'TEXT_TOO_SHORT: OCR 텍스트가 너무 짧습니다(총 58자 < 최소 80자).',\n",
       "  'location': 'page1:snippet=\"근로계약서 및 서약서 본 근로계약은 회사와 근로자 간의 근로관계를 명확히 하기 위하여 체결되며, 근로자는\"'},\n",
       " 'file_name': 'labor_rules_or_contract',\n",
       " 'processed_at': '2026-01-20T04:47:31Z'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/samples/S/근로계약서_서약서_지적사항강화_샘플2.docx\"\n",
    "slot_name = \"labor_rules_or_contract\"\n",
    "\n",
    "parsed = handle_file(path, slot_name)\n",
    "parsed\n",
    "\n",
    "validation = validate_unstructured_slot(parsed)\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9dec1254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'FAIL',\n",
       " 'error': {'code': 'VALIDATION_FAILED',\n",
       "  'message': \"TEXT_TOO_SHORT: OCR 텍스트가 너무 짧습니다(총 79자 < 최소 80자).; 필수 키워드 매칭 실패(최소 1개 필요): 누락=['아동노동', '강제노동', '금지']\",\n",
       "  'location': 'page1:snippet=\"강령 (Code of Conduct) 제1조 목적 대표이사 서명: __________________________ 시행일: 2025-01-01\"'},\n",
       " 'file_name': 'no_child_forced_labor_policy',\n",
       " 'processed_at': '2026-01-20T04:47:31Z'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/samples/S/윤리강령_샘플_아동강제노동포함2.docx\"\n",
    "slot_name = \"no_child_forced_labor_policy\"\n",
    "\n",
    "parsed = handle_file(path, slot_name)\n",
    "parsed\n",
    "\n",
    "validation = validate_unstructured_slot(parsed)\n",
    "validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
